{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "本例使用的是“FrozenLake-v0\"，一个4*4表格。\n",
    "Agent需要找到从出发点到目标点的最优路径。\n",
    "S表示起始位置，G表示目标位置，F表示可通行的Frozen冰面，H表示危险的Hole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "状态空间共有16格，从左到右编号，第一行是0，1，2，3.\n",
    "动作有4种，每个动作以一定概率P到达临近区域，并不是典型的上下左右移动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 0, 0.0, False),\n",
       " (0.3333333333333333, 5, 0.0, True),\n",
       " (0.3333333333333333, 2, 0.0, False)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[1][1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "转移概率矩阵是一个二维字典，返回可能的结果List.\n",
    "List中每个元素代表（转移概率p，下一个状态s'，当前步回报r，是否结束end）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=1.0, max_iterations=10000, threshold=1e-10):\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        updated_value_table = np.copy(value_table)\n",
    "\n",
    "        for state in range(env.observation_space.n):\n",
    "            Q_values = np.zeros(env.action_space.n)\n",
    "\n",
    "            for action in range(env.action_space.n):    \n",
    "                next_rewards = []\n",
    "                \n",
    "                for trans_prob, next_state, reward, _ in env.P[state][action]: # env.P[s][a] -> list\n",
    "                    next_state_reward = trans_prob * (reward + gamma * updated_value_table[next_state])\n",
    "                    next_rewards.append(next_state_reward)\n",
    "                    \n",
    "                Q_values[action] = sum(next_rewards)\n",
    "\n",
    "            value_table[state] = np.max(Q_values)\n",
    "            policy[state] = np.argmax(Q_values)\n",
    "            \n",
    "        if np.sum(np.fabs(updated_value_table - value_table)) <= threshold:\n",
    "            print(\"Value iteration converged at iteration %d.\" %(i+1))\n",
    "            break\n",
    "            \n",
    "    return policy, value_table"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "没有使用策略。\n",
    "遍历计算所有Q(s,a)，根据最大值输出V(s)和策略policy；其中V(s)会用于下一轮Q(s,a)的计算。\n",
    "每轮迭代中，V(s)和Q(s,a)都会更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged at iteration 877.\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, optimal_values = value_iteration(env=env, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph(values, width):\n",
    "    for i in range(0, len(values), width):\n",
    "        print(values[i: i+width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  3.  3.  3.]\n",
      "[ 0.  0.  0.  0.]\n",
      "[ 3.  1.  0.  0.]\n",
      "[ 0.  2.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "print_graph(optimal_policy, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.82352941  0.82352941  0.82352941  0.82352941]\n",
      "[ 0.82352941  0.          0.52941176  0.        ]\n",
      "[ 0.82352941  0.82352941  0.76470588  0.        ]\n",
      "[ 0.          0.88235294  0.94117647  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print_graph(optimal_values, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma=1.0, max_iteration=10000, threshold=1e-10):\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for i in range(max_iteration):\n",
    "        updated_value_table = np.copy(value_table)\n",
    "        \n",
    "        for state in range(env.observation_space.n):\n",
    "            action = policy[state]\n",
    "            next_rewards = []\n",
    "            \n",
    "            for trans_prob, next_state, reward, _ in env.P[state][action]: \n",
    "                next_state_reward = trans_prob * (reward + gamma * updated_value_table[next_state])\n",
    "                next_rewards.append(next_state_reward)\n",
    "                \n",
    "            Q_action = sum(next_rewards)\n",
    "            value_table[state] = Q_action\n",
    "        \n",
    "        if np.sum(np.fabs(updated_value_table - value_table)) <= threshold:\n",
    "            print(\"Policy evaluation converged at iteration %d.\" %(i+1))\n",
    "            break\n",
    "            \n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "需要指定一个策略，评估当前策略下的V(s)。\n",
    "该策略中action唯一确定，相当于pi(a|s)=1，所以 V(s) = sum(pi(a|s) * Q(s,a)) = Q(s,a)\n",
    "每次策略更新后，Value表格都要清零，重新开始计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, value_table, gamma=1.0):\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for state in range(env.observation_space.n):\n",
    "        Q_values = np.zeros(env.action_space.n)\n",
    "        \n",
    "        for action in range(env.action_space.n):\n",
    "            next_rewards = []\n",
    "            \n",
    "            for trans_prob, next_state, reward, _ in env.P[state][action]: \n",
    "                next_state_reward = trans_prob * (reward + gamma * value_table[next_state])\n",
    "                next_rewards.append(next_state_reward)\n",
    "                \n",
    "            Q_values[action] = sum(next_rewards)\n",
    "        \n",
    "        policy[state] = np.argmax(Q_values)\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "遍历计算每个Q(s,a)，从中选取Q值最大的action作为新策略: policy[state] = np.argmax(Q_values)\n",
    "计算Q(s,a)时使用了迭代收敛的value_table，所以必须在policy_evalution函数完成之后。\n",
    "不需要迭代，一次计算完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, max_iteration=10000):\n",
    "    policy = np.random.randint(0, 4, size=env.observation_space.n)\n",
    "    \n",
    "    for i in range(max_iteration):\n",
    "        value_table = policy_evaluation(env, policy)\n",
    "        new_policy = policy_improvement(env, value_table)\n",
    "        \n",
    "        if np.all(policy == new_policy):\n",
    "            print(\"Policy iteration converged at iteration %d.\" %(i+1))\n",
    "            break\n",
    "        else:\n",
    "            policy = new_policy\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "初始一个随机策略，评估策略并不断改进，至策略不再改变时迭代结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation converged at iteration 78.\n",
      "Policy evaluation converged at iteration 534.\n",
      "Policy evaluation converged at iteration 415.\n",
      "Policy evaluation converged at iteration 701.\n",
      "Policy evaluation converged at iteration 879.\n",
      "Policy iteration converged at iteration 5.\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = policy_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  3.  3.  3.]\n",
      "[ 0.  0.  0.  0.]\n",
      "[ 3.  1.  0.  0.]\n",
      "[ 0.  2.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "print_graph(optimal_policy, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_py36",
   "language": "python",
   "name": "openai_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
